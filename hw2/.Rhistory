F1 = 0
#kfold
for(j in 1: 10){
#define data row for training
start_pos = 100*(j -1) + 1
end_pos = 100*j
data_sample = start_pos:end_pos
#split training and test data
xtrain = german_new[-data_sample,feature]
ytrain = german$V21[-data_sample]
xtest = german_new[data_sample,feature]
ytest = german$V21[data_sample]
#fit the logisitc model
ml = glm(V21~.,family=binomial,data=data.frame(V21=ytrain,xtrain))
#predict
ptest = predict(ml,newdata=data.frame(xtest),type="response")
#use floor function to clamp the value to 0 or 1
btest=floor(ptest+0.5)
#get confusion matrix
conf.matrix = table(ytest,btest)
#get precision
precision = precision + conf.matrix[2,2] / (conf.matrix[2,2]+conf.matrix[1,2])
#get accuracy
accuracy = accuracy + (conf.matrix[1,1] + conf.matrix[2,2]) / 100
#get recall
recall = recall + conf.matrix[2,2] / (conf.matrix[2,2]+conf.matrix[2,1])
#get F1
F1 = F1 + 2 * precision * recall / (precision + recall)
}
if(recall/10 > best_recall){
best_feature = feature
best_conf.matrix = conf.matrix
best_recall = recall/10
best_vary = i
best_precision = precision/10
best_F1 = F1/10
best_accuracy = accuracy/10
}
}
Curve.Lift(ptest,ytest)
Curve.Roc(ptest,ytest)
print(best_conf.matrix)
print(rbind(best_recall,best_precision,best_F1,best_accuracy))
return(best_feature)
}
feature.9 = my_model(0.9)
feature.8 = my_model(0.8)
feature.7 = my_model(0.7)
feature.7
par(mfrow=c(2,3))
cat_vals = c("V1","V7","V10","V15","V19","V21")
ggplot(german,aes(x = V21,fill = V1)) + geom_bar(stat = "count") +facet_grid(.~V1)
ggplot(german,aes(x = V21,fill = V7)) + geom_histogram() +facet_grid(.~V7)
ggplot(german,aes(x = V21,fill = V10)) + geom_bar(stat = "count") +facet_grid(.~V10)
ggplot(german,aes(x = V21,fill = V15)) + geom_bar(stat = "count") +facet_grid(.~V15)
ggplot(german,aes(x = V21,fill = V19)) + geom_bar(stat = "count") +facet_grid(.~V19)
ggplot(german,aes(x = V21,fill = V7)) + geom_bar(stat = "count") +facet_grid(.~V7)
best_model = glm(V21~.,family=binomial,data=data.frame(V21=german$V21,german_new[,feature.9]))
best_model = glm(V21~.,family=binomial,data=data.frame(V21=german$V21,german_new[,feature.9]))
summary(best_model)
order(feature.9)
feature.9
sort(feature.9)
best_model = glm(V21~.,family=binomial,data=data.frame(V21=german$V21,german_new[,sort(feature.9)])
summary(best_model)
sort(feature.9)
best_model = glm(V21~.,family=binomial,data=data.frame(V21=german$V21,german_new[,c(sort(feature.9))])
summary(best_model)
best_model = glm(V21~.,family=binomial,data=data.frame(V21=german$V21,german_new[,c(sort(feature.9))]))
summary(best_model)
best_model = glm(V21~.,family=binomial,data=data.frame(V21=german$V21,german_new[,sort(feature.9)]))
summary(best_model)
#For the best model, compute the odds ratio and interpret the effect of each predictors.
exp(best_model$coefficients)
my_model<-function(proportion){
set.seed(88)
fold_length = nrow(german)/10
precision = 0
accuracy = 0
recall = 0
F1 = 0
best_recall = 0
best_vary = c()
best_precision = 0
best_accuracy = 0
best_F1 = 0
best_feature = c()
german_new = data.frame(german_new)
#define the proportion of predictors taken into training
for(i in c(rep(proportion,5))){
#define feature for training
feature = sample(1:length(german_new),floor(length(german_new) * i))
accuracy = 0
precision = 0
recall = 0
F1 = 0
#kfold
for(j in 1: 10){
#define data row for training
start_pos = 90*(j -1) + 1
end_pos = 90*j
data_sample = start_pos:end_pos
#split training and test data
xtrain = german_new[-data_sample,feature]
ytrain = german$V21[-data_sample]
xtest = german_new[data_sample,feature]
ytest = german$V21[data_sample]
#fit the logisitc model
ml = glm(V21~.,family=binomial,data=data.frame(V21=ytrain,xtrain))
#predict
ptest = predict(ml,newdata=data.frame(xtest),type="response")
#use floor function to clamp the value to 0 or 1
btest=floor(ptest+0.5)
#get confusion matrix
conf.matrix = table(ytest,btest)
#get precision
precision = precision + conf.matrix[2,2] / (conf.matrix[2,2]+conf.matrix[1,2])
#get accuracy
accuracy = accuracy + (conf.matrix[1,1] + conf.matrix[2,2]) / 100
#get recall
recall = recall + conf.matrix[2,2] / (conf.matrix[2,2]+conf.matrix[2,1])
#get F1
F1 = F1 + 2 * precision * recall / (precision + recall)
}
if(recall/10 > best_recall){
best_feature = feature
best_conf.matrix = conf.matrix
best_recall = recall/10
best_vary = i
best_precision = precision/10
best_F1 = F1/10
best_accuracy = accuracy/10
}
}
best_model = glm(V21~.,family=binomial,data=data.frame(V21=german$V21,german_new[,sort(feature.9)]))
ptest = predict(ml,newdata=data.frame(german_new[901:1000,]),type="response")
Curve.Lift(ptest,german$V21[901:1000])
Curve.Roc(ptest,german$V21[901:1000])
print(best_conf.matrix)
print(rbind(best_recall,best_precision,best_F1,best_accuracy))
return(best_feature)
}
feature.1 = my_model(1)
feature.1 = my_model(1)
#ROC
library(ROCR)
library(pROC)
Curve.Roc<- function(ptest,ytest){
## ROC for hold-out period
data = data.frame(predictions=ptest,labels=ytest)
## pred: function to create prediction objects
pred <- prediction(data$predictions,data$labels)
## perf: creates the input to be plotted
## sensitivity (TPR) and one minus specificity (FPR)
perf <- performance(pred, "sens", "fpr")
plot(perf)
area <- auc(data$labels,data$predictions)
area <- format(round(area, 4), nsmall = 4)
text(x=0.8, y=0.1, labels = paste("AUC =", area))
segments(x0=0, y0=0, x1=1, y1=1, col="gray", lty=2)
}
install.packages("pROC")
install.packages("pROC")
#ROC
library(ROCR)
library(pROC)
Curve.Roc<- function(ptest,ytest){
## ROC for hold-out period
data = data.frame(predictions=ptest,labels=ytest)
## pred: function to create prediction objects
pred <- prediction(data$predictions,data$labels)
## perf: creates the input to be plotted
## sensitivity (TPR) and one minus specificity (FPR)
perf <- performance(pred, "sens", "fpr")
plot(perf)
area <- auc(data$labels,data$predictions)
area <- format(round(area, 4), nsmall = 4)
text(x=0.8, y=0.1, labels = paste("AUC =", area))
segments(x0=0, y0=0, x1=1, y1=1, col="gray", lty=2)
}
feature.9 = my_model(0.9)
feature.1 = my_model(1)
feature.8 = my_model(0.8)
feature.7 = my_model(0.7)
library(ggplot2)
#input data from url
data.url = 'http://www.yurulin.com/class/spring2018_datamining/data/german_credit_data.csv'
german = read.csv(file = data.url,head = TRUE, sep = ',')
#Explore the dataset, and generate both statistical and graphical summary with respect to the numerical and categorical variables. (Consider only the following variables in this exploration: V1, V2, V5, V7, V10, V11, V13, V15, V16, V19 and V21.)
#Qa) Generate a summary table for the data. For each numerical variable, list: variable name, mean, median, 1st quartile, 3rd quartile, and standard deviation.
#apply summary table
numeric_values = c("V2","V5","V11","V13","V16")
summary_table = apply(german[,numeric_values],2,summary)
sd_table = apply(german[,numeric_values],2,sd)
result_table = rbind(summary_table,sd_table)#通过行bind列
result_table = t(round(result_table,2))#transform table
result_table
#For numerical variables, plot the density distribution. Describe whether the variable has a normal distribution or certain type of skew distribution.
par(mfrow=c(2,3))
for (numeric_value in numeric_values) {
plot(density(german[,numeric_value]),xlab = numeric_value, main = numeric_value)
}
#c) For each categorical predictor, generate the conditional histogram plot of response variable.
#hint: E.g., you can use facet_grid in ggplot.
par(mfrow=c(2,3))
cat_vals = c("V1","V7","V10","V15","V19","V21")
ggplot(german,aes(x = V21,fill = V1)) + geom_bar(stat = "count") +facet_grid(.~V1)
ggplot(german,aes(x = V21,fill = V7)) + geom_bar(stat = "count") +facet_grid(.~V7)
ggplot(german,aes(x = V21,fill = V10)) + geom_bar(stat = "count") +facet_grid(.~V10)
ggplot(german,aes(x = V21,fill = V15)) + geom_bar(stat = "count") +facet_grid(.~V15)
ggplot(german,aes(x = V21,fill = V19)) + geom_bar(stat = "count") +facet_grid(.~V19)
#Apply logistic regression analysis to predict V21. Evaluate the models through cross-validation and on holdout samples. Interpret the effect of the predictors.
#Implement a 10-fold cross-validation scheme by splitting the data into training and testing sets. Use the training set to train a logistic regression model to predict the response variable. Examine the performance of different models by varing the number of predictors. Report the performance of the models on testing set using proper measures (accuracy, precision, recall, F1) and plots (ROC, lift).
library(car)
suppressMessages( library(textir) )
#recode V21 = 2 to 0
german$V21 = recode(german$V21,"1=1;else=0")
#recode qualitive value
numerical_col = c(2,5,8,11,13,16,18)
data_numerical = german[,numerical_col]
data_qualitive = german[,-numerical_col]
#recode qualitive value
data_transform = model.matrix(V21~.,data=data_qualitive)[,-1]
#normalize numerical value
#data_normalize = scale(data_numerical[,])
german_new <- cbind(data_numerical,data_transform)
#ROC
library(ROCR)
library(pROC)
Curve.Roc<- function(ptest,ytest){
## ROC for hold-out period
data = data.frame(predictions=ptest,labels=ytest)
## pred: function to create prediction objects
pred <- prediction(data$predictions,data$labels)
## perf: creates the input to be plotted
## sensitivity (TPR) and one minus specificity (FPR)
perf <- performance(pred, "sens", "fpr")
plot(perf)
area <- auc(data$labels,data$predictions)
area <- format(round(area, 4), nsmall = 4)
text(x=0.8, y=0.1, labels = paste("AUC =", area))
segments(x0=0, y0=0, x1=1, y1=1, col="gray", lty=2)
}
#Lift
Curve.Lift<-function(ptest,ytest){
df=cbind(ptest,ytest)
rank.df=as.data.frame(df[order(ptest,decreasing=TRUE),])
colnames(rank.df) = c('predicted','actual')
baserate=mean(ytest)
## calculating the lift
## cumulative 1's sorted by predicted values
## cumulative 1's using the average success prob from evaluation set
n.test = 100
ax=dim(n.test)
ay.base=dim(n.test)
ay.pred=dim(n.test)
ax[1]=1
ay.base[1]=baserate
ay.pred[1]=rank.df$actual[1]
for (i in 2:n.test) {
ax[i]=i
ay.base[i]=baserate*i ## uniformly increase with rate xbar
ay.pred[i]=ay.pred[i-1]+rank.df$actual[i]
}
df=cbind(rank.df,ay.pred,ay.base)
plot(ax,ay.pred,xlab="number of cases",ylab="number of successes",main="Lift: Cum successes sorted by pred val/success prob")
points(ax,ay.base,type="l")
}
my_model<-function(proportion){
set.seed(88)
fold_length = nrow(german)/10
precision = 0
accuracy = 0
recall = 0
F1 = 0
best_recall = 0
best_vary = c()
best_precision = 0
best_accuracy = 0
best_F1 = 0
best_feature = c()
german_new = data.frame(german_new)
#define the proportion of predictors taken into training
for(i in c(rep(proportion,5))){
#define feature for training
feature = sample(1:length(german_new),floor(length(german_new) * i))
accuracy = 0
precision = 0
recall = 0
F1 = 0
#kfold
for(j in 1: 10){
#define data row for training
start_pos = 90*(j -1) + 1
end_pos = 90*j
data_sample = start_pos:end_pos
#split training and test data
xtrain = german_new[-data_sample,feature]
ytrain = german$V21[-data_sample]
xtest = german_new[data_sample,feature]
ytest = german$V21[data_sample]
#fit the logisitc model
ml = glm(V21~.,family=binomial,data=data.frame(V21=ytrain,xtrain))
#predict
ptest = predict(ml,newdata=data.frame(xtest),type="response")
#use floor function to clamp the value to 0 or 1
btest=floor(ptest+0.5)
#get confusion matrix
conf.matrix = table(ytest,btest)
#get precision
precision = precision + conf.matrix[2,2] / (conf.matrix[2,2]+conf.matrix[1,2])
#get accuracy
accuracy = accuracy + (conf.matrix[1,1] + conf.matrix[2,2]) / 100
#get recall
recall = recall + conf.matrix[2,2] / (conf.matrix[2,2]+conf.matrix[2,1])
#get F1
F1 = F1 + 2 * precision * recall / (precision + recall)
}
if(recall/10 > best_recall){
best_feature = feature
best_conf.matrix = conf.matrix
best_recall = recall/10
best_vary = i
best_precision = precision/10
best_F1 = F1/10
best_accuracy = accuracy/10
}
}
best_model = glm(V21~.,family=binomial,data=data.frame(V21=german$V21,german_new[,sort(feature.9)]))
ptest = predict(ml,newdata=data.frame(german_new[901:1000,]),type="response")
Curve.Lift(ptest,german$V21[901:1000])
Curve.Roc(ptest,german$V21[901:1000])
print(best_conf.matrix)
print(rbind(best_recall,best_precision,best_F1,best_accuracy))
return(best_feature)
}
feature.1 = my_model(1)
feature.9 = my_model(0.9)
feature.8 = my_model(0.8)
feature.7 = my_model(0.7)
#compute the odds ratio and interpret the effect of each predictors.
best_model = glm(V21~.,family=binomial,data=data.frame(V21=german$V21,german_new[,sort(feature.9)]))
summary(best_model)
#For the best model, compute the odds ratio and interpret the effect of each predictors.
exp(best_model$coefficients)
my_model<-function(proportion){
set.seed(88)
fold_length = nrow(german)/10
precision = 0
accuracy = 0
recall = 0
F1 = 0
best_recall = 0
best_vary = c()
best_precision = 0
best_accuracy = 0
best_F1 = 0
best_feature = c()
german_new = data.frame(german_new)
#define the proportion of predictors taken into training
for(i in c(rep(proportion,5))){
#define feature for training
feature = sample(1:length(german_new),floor(length(german_new) * i))
accuracy = 0
precision = 0
recall = 0
F1 = 0
#kfold
for(j in 1: 10){
#define data row for training
#set 900 for train and validate, 901-1000 for test
start_pos = 90*(j -1) + 1
end_pos = 90*j
data_sample = start_pos:end_pos
#split training and test data
xtrain = german_new[-data_sample,feature]
ytrain = german$V21[-data_sample]
xtest = german_new[data_sample,feature]
ytest = german$V21[data_sample]
#fit the logisitc model
ml = glm(V21~.,family=binomial,data=data.frame(V21=ytrain,xtrain))
#predict
ptest = predict(ml,newdata=data.frame(xtest),type="response")
#use floor function to clamp the value to 0 or 1
btest=floor(ptest+0.5)
#get confusion matrix
conf.matrix = table(ytest,btest)
#get precision
precision = precision + conf.matrix[2,2] / (conf.matrix[2,2]+conf.matrix[1,2])
#get accuracy
accuracy = accuracy + (conf.matrix[1,1] + conf.matrix[2,2]) / 100
#get recall
recall = recall + conf.matrix[2,2] / (conf.matrix[2,2]+conf.matrix[2,1])
#get F1
F1 = F1 + 2 * precision * recall / (precision + recall)
}
if(recall/10 > best_recall){
best_feature = feature
best_conf.matrix = conf.matrix
best_recall = recall/10
best_vary = i
best_precision = precision/10
best_F1 = F1/10
best_accuracy = accuracy/10
}
}
best_model = glm(V21~.,family=binomial,data=data.frame(V21=german$V21,german_new[,sort(best_feature)]))
#set 901 - 1000 for test
ptest = predict(ml,newdata=data.frame(german_new[901:1000,]),type="response")
Curve.Lift(ptest,german$V21[901:1000])
Curve.Roc(ptest,german$V21[901:1000])
print(best_conf.matrix)
print(rbind(best_recall,best_precision,best_F1,best_accuracy))
return(best_feature)
}
feature.1 = my_model(1)
feature.9 = my_model(0.9)
my_model<-function(proportion){
set.seed(88)
fold_length = nrow(german)/10
precision = 0
accuracy = 0
recall = 0
F1 = 0
best_recall = 0
best_vary = c()
best_precision = 0
best_accuracy = 0
best_F1 = 0
best_feature = c()
german_new = data.frame(german_new)
#define the proportion of predictors taken into training
for(i in c(rep(proportion,5))){
#define feature for training
feature = sample(1:length(german_new),floor(length(german_new) * i))
accuracy = 0
precision = 0
recall = 0
F1 = 0
#kfold
for(j in 1: 10){
#define data row for training
#set 900 for train and validate, 901-1000 for test
start_pos = 90*(j -1) + 1
end_pos = 90*j
data_sample = start_pos:end_pos
#split training and test data
xtrain = german_new[-data_sample,feature]
ytrain = german$V21[-data_sample]
xtest = german_new[data_sample,feature]
ytest = german$V21[data_sample]
#fit the logisitc model
ml = glm(V21~.,family=binomial,data=data.frame(V21=ytrain,xtrain))
#predict
ptest = predict(ml,newdata=data.frame(xtest),type="response")
#use floor function to clamp the value to 0 or 1
btest=floor(ptest+0.5)
#get confusion matrix
conf.matrix = table(ytest,btest)
#get precision
precision = precision + conf.matrix[2,2] / (conf.matrix[2,2]+conf.matrix[1,2])
#get accuracy
accuracy = accuracy + (conf.matrix[1,1] + conf.matrix[2,2]) / 100
#get recall
recall = recall + conf.matrix[2,2] / (conf.matrix[2,2]+conf.matrix[2,1])
#get F1
F1 = F1 + 2 * precision * recall / (precision + recall)
}
if(recall/10 > best_recall){
best_feature = feature
best_conf.matrix = conf.matrix
best_recall = recall/10
best_vary = i
best_precision = precision/10
best_F1 = F1/10
best_accuracy = accuracy/10
}
}
best_model = glm(V21~.,family=binomial,data=data.frame(V21=german$V21,german_new[,sort(best_feature)]))
#set 901 - 1000 for test
ptest = predict(best_model,newdata=data.frame(german_new[901:1000,]),type="response")
Curve.Lift(ptest,german$V21[901:1000])
Curve.Roc(ptest,german$V21[901:1000])
print(best_conf.matrix)
print(rbind(best_recall,best_precision,best_F1,best_accuracy))
return(best_feature)
}
feature.9 = my_model(0.9)
feature.8 = my_model(0.8)
feature.7 = my_model(0.7)
#compute the odds ratio and interpret the effect of each predictors.
best_model = glm(V21~.,family=binomial,data=data.frame(V21=german$V21,german_new[,sort(feature.9)]))
summary(best_model)
#For the best model, compute the odds ratio and interpret the effect of each predictors.
exp(best_model$coefficients)
best_model = glm(V21~.,family=binomial,data=data.frame(V21=german$V21,german_new[,sort(feature.9)]))
summary(best_model)
#For the best model, compute the odds ratio and interpret the effect of each predictors.
exp(best_model$coefficients)
par(mfrow=c(2,3))
cat_vals = c("V1","V7","V10","V15","V19","V21")
ggplot(german,aes(x = V21,fill = V1)) + geom_histogram(binwidth = 0.5) +facet_grid(.~V1)
library(ggplot2)
#input data from url
data.url = 'http://www.yurulin.com/class/spring2018_datamining/data/german_credit_data.csv'
german = read.csv(file = data.url,head = TRUE, sep = ',')
par(mfrow=c(2,3))
cat_vals = c("V1","V7","V10","V15","V19","V21")
ggplot(german,aes(x = V21,fill = V1)) + geom_histogram(binwidth = 0.5) +facet_grid(.~V1)
par(mfrow=c(2,3))
cat_vals = c("V1","V7","V10","V15","V19","V21")
ggplot(german,aes(x = V21,fill = V1)) + geom_bar(stat = "count") +facet_grid(.~V1)
