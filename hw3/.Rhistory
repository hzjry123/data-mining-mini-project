cost = tuned[['best.parameters']]$cost
#set.seed(12345) # set the seed so you can get exactly the same results whenever you run the code
do.classification <- function(train.set, test.set,
cl.name, knum,tune = F,verbose=F) {
## note: to plot ROC later, we want the raw probabilities,
## not binary decisions
switch(cl.name,
knn = { # here we test k=3; you should evaluate different k's
prob = knn(train.set[,-1], test.set[,-1], cl=train.set[,1], k = knum, prob=T)
prob = attr(prob,"prob")
prob
},
lr = { # logistic regression
#model = bayesglm(y~., family=binomial, data=data.frame(train.set),control=list(maxit=100))
model = glmnet(as.matrix(train.set[-ncol(train.set)]),as.matrix(train.set[ncol(train.set)]),family="binomial",alpha=0,lambda=1/100,  standardize=F,thresh =1e-8,intercept = F)
if (verbose) {
print(summary(model))
}
# prob = predict(model, newdata=test.set, type="response")
prob = predict(model, newx=as.matrix(test.set[-ncol(test.set)]), type="response")
#print(cbind(prob,as.character(test.set$y)))
prob
},
nb = {
model = naiveBayes(y~., data=train.set)
prob = predict(model, newdata=test.set, type="raw")
#print(cbind(prob,as.character(test.set$y)))
prob = prob[,2]/rowSums(prob) # renormalize the prob.
prob
},
dtree = {
model = rpart(y~., data=train.set)
if (verbose) {
print(summary(model)) # detailed summary of splits
printcp(model) # print the cross-validation results
plotcp(model) # visualize the cross-validation results
## plot the tree
plot(model, uniform=TRUE, main="Classification Tree")
text(model, use.n=TRUE, all=TRUE, cex=.8)
}
prob = predict(model, newdata=test.set)
if (tune) { # here we use the default tree,
## you should evaluate different size of tree
## prune the tree
print("jinll\n")
pfit<- prune(model, cp=model$cptable[which.min(model$cptable[,"xerror"]),"CP"])
prob = predict(pfit, newdata=test.set)
## plot the pruned tree
#plot(pfit, uniform=TRUE,main="Pruned Classification Tree")
#text(pfit, use.n=TRUE, all=TRUE, cex=.8)
}
#print(cbind(prob,as.character(test.set$y)))
prob
},
svm.radial = {
#model = svm(y~., data=train.set, probability=T,scale = FALSE)
model = svm(y~., data = train.set, probability=T,
kernel="radial", gamma=0.01, cost=10,scale = FALSE)
if (0) { # fine-tune the model with different kernel and parameters
## evaluate the range of gamma parameter between 0.000001 and 0.1
## and cost parameter from 0.1 until 10
tuned <- tune.svm(y~., data = train.set,
kernel="radial",
gamma = 10^(-6:-1), cost = 10^(-1:2))
print(summary(tuned))
gamma = tuned[['best.parameters']]$gamma
cost = tuned[['best.parameters']]$cost
model = svm(y~., data = train.set, probability=T,
kernel="radial", gamma=gamma, cost=cost,scale = FALSE)
}
prob = predict(model, newdata=test.set, probability=T)
#prob = attr(prob,"probabilities")
#print(cbind(prob,as.character(test.set$y)))
#print(dim(prob))
#prob = prob[,which(colnames(prob)==1)]/rowSums(prob)
prob
},
svm.sigmoid = {
model = svm(y~., data=train.set, probability=T,scale = FALSE,kernel="sigmoid",gamma = 0.01, cost =10)
#model = svm(y~., data = train.set, probability=T,
#kernel="sigmoid", gamma=0.01, cost=10,coef0 = 0,scale = FALSE)
if (0) { # fine-tune the model with different kernel and parameters
## evaluate the range of gamma parameter between 0.000001 and 0.1
## and cost parameter from 0.1 until 10
tuned <- tune.svm(y~., data = train.set,
kernel="sigmoid",
gamma = 10^(-2:-1), cost = 10^(-1:2),coef0 = 10^(-1:1))
#print(summary(tuned))
gamma = tuned[['best.parameters']]$gamma
cost = tuned[['best.parameters']]$cost
coef0 = tuned[['best.parameters']]$coef0
model = svm(y~., data = train.set, probability=T,
kernel="sigmoid", gamma=gamma, cost=cost,coef0 = coef0,scale = FALSE)
}
prob = predict(model, newdata=test.set, probability=T)
#prob = attr(prob,"probabilities")
#print(cbind(prob,as.character(test.set$y)))
#print(dim(prob))
#prob = prob[,which(colnames(prob)==1)]/rowSums(prob)
prob
},
ada = {
model = ada(y~., data = train.set)
prob = predict(model, newdata=test.set, type='probs')
#print(cbind(prob,as.character(test.set$y)))
prob = prob[,2]/rowSums(prob)
prob
}
)
}
### main ###
dataset = load.data.pokemon()
dataset_new = load.data.clean(dataset)
#get predictions
pred.lr = k.fold.cv(dataset_new,'lr')
pred.knn1 = k.fold.cv(dataset_new,'knn',1)
pred.knn3 = k.fold.cv(dataset_new,'knn',3)
pred.knn5 = k.fold.cv(dataset_new,'knn',5)
pred.nb = k.fold.cv(dataset_new, 'nb')
pred.svm.radial = k.fold.cv(dataset_new, 'svm.radial',tune = T)
pred.svm.sigmoid = k.fold.cv(dataset_new, 'svm.sigmoid',tune = T)
pred.dtree.default = k.fold.cv(dataset_new, 'dtree')
pred.dtree.prune = k.fold.cv(dataset_new, 'dtree',tune = T)
#preds = cbind(pred.lr,pred.knn1,pred.knn3,pred.knn5,pred.nb)
#get measure matrix
mat.lr = get.measure.matrix(pred.lr,'lr')
mat.knn1 = get.measure.matrix(pred.knn1,'knn1')
mat.knn3 = get.measure.matrix(pred.knn3,'knn3')
mat.knn5 = get.measure.matrix(pred.knn5,'knn5')
mat.nb = get.measure.matrix(pred.nb,'nb')
mat.svm.radial = get.measure.matrix(pred.svm.radial,'svm radial')
mat.svm.sigmoid = get.measure.matrix(pred.svm.sigmoid,'svm sigmoid')
mat.dtree.default = get.measure.matrix(pred.dtree.default,'dtree default')
mat.dtree.prune = get.measure.matrix(pred.dtree.prune,'dtree prune')
#show measrure matrix
mat.all = cbind(mat.lr,mat.knn1,mat.knn3,mat.knn5,mat.nb,mat.svm.radial,mat.svm.sigmoid,mat.dtree.default,mat.dtree.prune)
mat.all
#k.fold.cv(dataset_new, 'dtree')
#my.classifier(dataset_new, cl.name='lr',do.cv=T)
# cl.name can take 'lr','knn','nb','dtree','svm','ada'
#set.seed(12345) # set the seed so you can get exactly the same results whenever you run the code
do.classification <- function(train.set, test.set,
cl.name, knum,tune = F,verbose=F) {
## note: to plot ROC later, we want the raw probabilities,
## not binary decisions
switch(cl.name,
knn = { # here we test k=3; you should evaluate different k's
prob = knn(train.set[,-1], test.set[,-1], cl=train.set[,1], k = knum, prob=T)
prob = attr(prob,"prob")
prob
},
lr = { # logistic regression
#model = bayesglm(y~., family=binomial, data=data.frame(train.set),control=list(maxit=100))
model = glmnet(as.matrix(train.set[-ncol(train.set)]),as.matrix(train.set[ncol(train.set)]),family="binomial",alpha=0,lambda=1/100,  standardize=F,thresh =1e-8,intercept = F)
if (verbose) {
print(summary(model))
}
# prob = predict(model, newdata=test.set, type="response")
prob = predict(model, newx=as.matrix(test.set[-ncol(test.set)]), type="response")
#print(cbind(prob,as.character(test.set$y)))
prob
},
nb = {
model = naiveBayes(y~., data=train.set)
prob = predict(model, newdata=test.set, type="raw")
#print(cbind(prob,as.character(test.set$y)))
prob = prob[,2]/rowSums(prob) # renormalize the prob.
prob
},
dtree = {
model = rpart(y~., data=train.set)
if (verbose) {
print(summary(model)) # detailed summary of splits
printcp(model) # print the cross-validation results
plotcp(model) # visualize the cross-validation results
## plot the tree
plot(model, uniform=TRUE, main="Classification Tree")
text(model, use.n=TRUE, all=TRUE, cex=.8)
}
prob = predict(model, newdata=test.set)
if (tune) { # here we use the default tree,
## you should evaluate different size of tree
## prune the tree
pfit<- prune(model, cp=model$cptable[which.min(model$cptable[,"xerror"]),"CP"])
prob = predict(pfit, newdata=test.set)
## plot the pruned tree
#plot(pfit, uniform=TRUE,main="Pruned Classification Tree")
#text(pfit, use.n=TRUE, all=TRUE, cex=.8)
}
#print(cbind(prob,as.character(test.set$y)))
prob
},
svm.radial = {
#model = svm(y~., data=train.set, probability=T,scale = FALSE)
model = svm(y~., data = train.set, probability=T,
kernel="radial", gamma=0.01, cost=10,scale = FALSE)
if (0) { # fine-tune the model with different kernel and parameters
## evaluate the range of gamma parameter between 0.000001 and 0.1
## and cost parameter from 0.1 until 10
tuned <- tune.svm(y~., data = train.set,
kernel="radial",
gamma = 10^(-6:-1), cost = 10^(-1:2))
print(summary(tuned))
gamma = tuned[['best.parameters']]$gamma
cost = tuned[['best.parameters']]$cost
model = svm(y~., data = train.set, probability=T,
kernel="radial", gamma=gamma, cost=cost,scale = FALSE)
}
prob = predict(model, newdata=test.set, probability=T)
#prob = attr(prob,"probabilities")
#print(cbind(prob,as.character(test.set$y)))
#print(dim(prob))
#prob = prob[,which(colnames(prob)==1)]/rowSums(prob)
prob
},
svm.sigmoid = {
model = svm(y~., data=train.set, probability=T,scale = FALSE,kernel="sigmoid",gamma = 0.01, cost =10)
#model = svm(y~., data = train.set, probability=T,
#kernel="sigmoid", gamma=0.01, cost=10,coef0 = 0,scale = FALSE)
if (0) { # fine-tune the model with different kernel and parameters
## evaluate the range of gamma parameter between 0.000001 and 0.1
## and cost parameter from 0.1 until 10
tuned <- tune.svm(y~., data = train.set,
kernel="sigmoid",
gamma = 10^(-2:-1), cost = 10^(-1:2),coef0 = 10^(-1:1))
#print(summary(tuned))
gamma = tuned[['best.parameters']]$gamma
cost = tuned[['best.parameters']]$cost
coef0 = tuned[['best.parameters']]$coef0
model = svm(y~., data = train.set, probability=T,
kernel="sigmoid", gamma=gamma, cost=cost,coef0 = coef0,scale = FALSE)
}
prob = predict(model, newdata=test.set, probability=T)
#prob = attr(prob,"probabilities")
#print(cbind(prob,as.character(test.set$y)))
#print(dim(prob))
#prob = prob[,which(colnames(prob)==1)]/rowSums(prob)
prob
},
ada = {
model = ada(y~., data = train.set)
prob = predict(model, newdata=test.set, type='probs')
#print(cbind(prob,as.character(test.set$y)))
prob = prob[,2]/rowSums(prob)
prob
}
)
}
## plot ROC
#result = data.frame(probs,actuals)
#labels = matrix(ROCR.simple$labels,
#nrow = length(ROCR.simple$labels), ncol = 2)
preds_list = list(get.predictions(pred.lr),get.predictions(pred.knn3),get.predictions(pred.nb),get.predictions(pred.dtree.default),get.predictions(pred.svm.radial))
#labels = do.call("cbind", replicate(ncol(preds), dataset_new$y, simplify = FALSE))
m <- length(preds_list)
actuals_list <- rep(list(dataset_new$y), m)
predss = prediction(preds_list,actuals_list)
perf = performance(predss, "tpr","fpr")
plot(perf,col = as.list(1:m), main = "Test Set ROC Curves")
legend(x = "bottomright",c("lr","knn3","nb","dtree default","svm radial"),fill = 1 : m)
mat.lr$auc
mat.lr[auc]
mat.lr['auc']
mat.lr
mat.lr[,'auc']
mat.lr[1,'auc']
mat.lr['auc']
mat.lr[1]
mat.lr[5]
counts <- table(mat.lr[5],mat.knn3[5])
barplot(counts, main="Car Distribution",
xlab="Number of Gears")
ggplot(aes(x = c('Svm','knn3'),y = c(mat.lr[5],mat.knn3[5]))) + geom_bar()
ggplot(aes(x = c('Svm','knn3'),y = c(mat.lr[5],mat.knn3[5]))) + geom_bar(stat = "count")
ggplot(aes(x = c('Svm','knn3'),y = c(mat.lr[5],mat.knn3[5]))) + geom_col()
ggplot(aes(c('Svm','knn3'),c(mat.lr[5],mat.knn3[5]))) + geom_col()
data.plot = data.frame(x = c('Svm','knn3'), y= c(mat.lr[5],mat.knn3[5]))
ggplot(aes()) + geom_col()
data.plot = data.frame(x = c('Svm','knn3'), y= c(mat.lr[5],mat.knn3[5]))
ggplot(data.plot,aes(x,y)) + geom_col()
data.plot = data.frame(x = c('Svm','knn3'), y= c(mat.lr[5],mat.knn3[5]))
ggplot(data.plot,aes(x,y),colours()) + geom_col()
data.plot = data.frame(x = c('Svm','knn3'), y= c(mat.lr[5],mat.knn3[5]))
ggplot(data.plot,aes(x,y),colours(c('red','blue'))) + geom_col()
data.plot = data.frame(x = c('Svm','knn3'), y= c(mat.lr[5],mat.knn3[5]))
ggplot(data.plot,aes(x,y,,colours(c('red','blue'))) + geom_col()
data.plot = data.frame(x = c('Svm','knn3'), y= c(mat.lr[5],mat.knn3[5]))
ggplot(data.plot,aes(x,y,,colours(c('red','blue')))) + geom_col()
data.plot = data.frame(x = c('Svm','knn3'), y= c(mat.lr[5],mat.knn3[5]))
ggplot(data.plot,aes(x,y,colours(c('red','blue')))) + geom_col()
data.plot = data.frame(x = c('Svm','knn3'), y= c(mat.lr[5],mat.knn3[5]))
ggplot(data.plot,aes(x,y)) + geom_bar(stat = "count")
data.plot = data.frame(x = c('Svm','knn3'), y= c(mat.lr[5],mat.knn3[5]))
ggplot(data.plot,aes(x,y)) + geom_bar(stat = "identity",aes(fill = type))
data.plot = data.frame(x = c('Svm','knn3'), y= c(mat.lr[5],mat.knn3[5]))
ggplot(data.plot,aes(x,y)) + geom_bar(stat = "identity")
data.plot = data.frame(x = c('Svm','knn3'), y= c(mat.lr[5],mat.knn3[5]))
ggplot(data.plot,aes(x,y),fill = x) + geom_bar(stat = "identity")
data.plot = data.frame(x = c('Svm','knn3'), y= c(mat.lr[5],mat.knn3[5]))
ggplot(data.plot,aes(x,y),fill = model) + geom_bar(stat = "identity")
data.plot = data.frame(model = c('Svm','knn3'), y= c(mat.lr[5],mat.knn3[5]))
ggplot(data.plot,aes(x,y),fill = model) + geom_bar(stat = "identity")
data.plot = data.frame(model = c('Svm','knn3'), y= c(mat.lr[5],mat.knn3[5]))
ggplot(data.plot,aes(model,y),fill = model) + geom_bar(stat = "identity")
data.plot = data.frame(model = c('Svm','knn3'), auc = c(mat.lr[5],mat.knn3[5]))
ggplot(data.plot,aes(model,y),fill = model) + geom_bar(stat = "identity")
data.plot = data.frame(model = c('Svm','knn3'), auc = c(mat.lr[5],mat.knn3[5]))
ggplot(data.plot,aes(model,auc),fill = model) + geom_bar(stat = "identity")
data.plot = data.frame(model = c('Svm','knn3'), auc = c(mat.lr[5],mat.knn3[5]))
ggplot(data.plot,aes(model,auc,fill = model)) + geom_bar(stat = "identity")
data.plot = data.frame(model = c('lr','knn1','knn3','knn5','nb','svm.radial','svm.sigmoid','dtree.default','dtree.prune'), auc = c(mat.lr[5],mat.knn1[5],mat.knn3[5].mat.knn5[5],mat.nb[5],mat.svm.radial[5],mat.svm.sigmoid[5],mat.dtree.default[5],mat.dtree.prune][5]))
data.plot = data.frame(model = c('lr','knn1','knn3','knn5','nb','svm.radial','svm.sigmoid','dtree.default','dtree.prune'), auc = c(mat.lr[5],mat.knn1[5],mat.knn3[5].mat.knn5[5],mat.nb[5],mat.svm.radial[5],mat.svm.sigmoid[5],mat.dtree.default[5],mat.dtree.prune[5]))
data.plot = data.frame(model = c('lr','knn1','knn3','knn5','nb','svm.radial','svm.sigmoid','dtree.default','dtree.prune'), auc = c(mat.lr[5],mat.knn1[5],mat.knn3[5],mat.knn5[5],mat.nb[5],mat.svm.radial[5],mat.svm.sigmoid[5],mat.dtree.default[5],mat.dtree.prune[5]))
ggplot(data.plot,aes(model,auc,fill = model)) + geom_bar(stat = "identity")
data.plot = data.frame(model = c('lr','knn1','knn3','knn5','nb','svm.radial','svm.sigmoid','dtree.default','dtree.prune'), auc = c(mat.lr[5],mat.knn1[5],mat.knn3[5],mat.knn5[5],mat.nb[5],mat.svm.radial[5],mat.svm.sigmoid[5],mat.dtree.default[5],mat.dtree.prune[5]))
ggplot(data.plot,aes(model,auc)) + geom_bar(stat = "identity")
data.plot = data.frame(model = c('lr','knn1','knn3','knn5','nb','svm.radial','svm.sigmoid','dtree.default','dtree.prune'), auc = c(mat.lr[5],mat.knn1[5],mat.knn3[5],mat.knn5[5],mat.nb[5],mat.svm.radial[5],mat.svm.sigmoid[5],mat.dtree.default[5],mat.dtree.prune[5]))
ggplot(data.plot,aes(model,auc,fill = model)) + geom_bar(stat = "identity")
data.plot = data.frame(model = c('lr','knn1','knn3','knn5','nb','svm.radial','svm.sigmoid','dtree.default','dtree.prune'), fscore = c(mat.lr[4],mat.knn1[4],mat.knn3[4],mat.knn5[4],mat.nb[4],mat.svm.radial[4],mat.svm.sigmoid[4],mat.dtree.default[4],mat.dtree.prune[4]))
ggplot(data.plot,aes(model,fscore,fill = model)) + geom_bar(stat = "identity")
data.plot = data.frame(model = c('lr','knn1','knn3','knn5','nb','svm.radial','svm.sigmoid','dtree.default','dtree.prune'), auc = c(mat.lr[5],mat.knn1[5],mat.knn3[5],mat.knn5[5],mat.nb[5],mat.svm.radial[5],mat.svm.sigmoid[5],mat.dtree.default[5],mat.dtree.prune[5]))
ggplot(data.plot,aes(model,auc,fill = model,main = "AUC")) + geom_bar(stat = "identity")
data.plot = data.frame(model = c('lr','knn1','knn3','knn5','nb','svm.radial','svm.sigmoid','dtree.default','dtree.prune'), fscore = c(mat.lr[4],mat.knn1[4],mat.knn3[4],mat.knn5[4],mat.nb[4],mat.svm.radial[4],mat.svm.sigmoid[4],mat.dtree.default[4],mat.dtree.prune[4]))
ggplot(data.plot,aes(model,fscore,fill = model)) + geom_bar(stat = "identity") + ggtitle("FScore")
data.plot = data.frame(model = c('lr','knn1','knn3','knn5','nb','svm.radial','svm.sigmoid','dtree.default','dtree.prune'), auc = c(mat.lr[5],mat.knn1[5],mat.knn3[5],mat.knn5[5],mat.nb[5],mat.svm.radial[5],mat.svm.sigmoid[5],mat.dtree.default[5],mat.dtree.prune[5]))
ggplot(data.plot,aes(model,auc,fill = model)) + geom_bar(stat = "identity") + ggtitle("AUC Bar")
library(glmnet)#for penalize logistic regression
library("arm")
library(class) #for knn
library(MASS) # for the example dataset
library(plyr) # for recoding data
library(ROCR) # for plotting roc
library(e1071) # for NB and SVM
library(rpart) # for decision tree
library(ada) # for adaboost
#for kNN, you may include kNN-1, kNN-3, kNN-5.
#For decision tree, you may include the default tree, and a
#tree after pruning. For SVM, you may include different
#kernels and gamma/cost parameters.
do.classification <- function(train.set, test.set,
cl.name, knum,tune = F,verbose=F) {
## note: to plot ROC later, we want the raw probabilities,
## not binary decisions
switch(cl.name,
knn = {#set key by pass knum
prob = knn(train.set[,-1], test.set[,-1], cl=train.set[,1], k = knum, prob=T)
prob = attr(prob,"prob")
prob
},
lr = { # logistic regression
#set glmnet for penalize logistic regression to avoid overfitting
model = glmnet(as.matrix(train.set[-ncol(train.set)]),as.matrix(train.set[ncol(train.set)]),family="binomial",alpha=0,lambda=1/100,  standardize=F,thresh =1e-8,intercept = F)
if (verbose) {
print(summary(model))
}
prob = predict(model, newx=as.matrix(test.set[-ncol(test.set)]), type="response")
prob
},
nb = {#naive bayes model
model = naiveBayes(y~., data=train.set)
prob = predict(model, newdata=test.set, type="raw")
prob = prob[,2]/rowSums(prob) # renormalize the prob.
prob
},
dtree = {
model = rpart(y~., data=train.set)
if (verbose) {
print(summary(model)) # detailed summary of splits
printcp(model) # print the cross-validation results
plotcp(model) # visualize the cross-validation results
## plot the tree
plot(model, uniform=TRUE, main="Classification Tree")
text(model, use.n=TRUE, all=TRUE, cex=.8)
}
prob = predict(model, newdata=test.set)
if (tune) {
## prune the tree
pfit<- prune(model, cp=model$cptable[which.min(model$cptable[,"xerror"]),"CP"])
prob = predict(pfit, newdata=test.set)
## plot the pruned tree
#plot(pfit, uniform=TRUE,main="Pruned Classification Tree")
#text(pfit, use.n=TRUE, all=TRUE, cex=.8)
}
#print(cbind(prob,as.character(test.set$y)))
prob
},
svm.radial = {#svm with kernel radial
#alredy get best gamma and cost in grid search
model = svm(y~., data = train.set, probability=T,
kernel="radial", gamma=0.01, cost=10,scale = FALSE)
if (0) { # fine-tune the model with different kernel and parameters
## evaluate the range of gamma parameter between 0.000001 and 0.1
## and cost parameter from 0.1 until 10
tuned <- tune.svm(y~., data = train.set,
kernel="radial",
gamma = 10^(-6:-1), cost = 10^(-1:2))
print(summary(tuned))
gamma = tuned[['best.parameters']]$gamma
cost = tuned[['best.parameters']]$cost
model = svm(y~., data = train.set, probability=T,
kernel="radial", gamma=gamma, cost=cost,scale = FALSE)
}
prob = predict(model, newdata=test.set, probability=T)
prob
},
svm.sigmoid = {#svm with kernel sigmoid
model = svm(y~., data=train.set, probability=T,scale = FALSE,kernel="sigmoid",gamma = 0.01, cost =10)
if (0) { # fine-tune the model with different kernel and parameters
## evaluate the range of gamma parameter between 0.000001 and 0.1
## and cost parameter from 0.1 until 10
tuned <- tune.svm(y~., data = train.set,
kernel="sigmoid",
gamma = 10^(-2:-1), cost = 10^(-1:2),coef0 = 10^(-1:1))
#print(summary(tuned))
gamma = tuned[['best.parameters']]$gamma
cost = tuned[['best.parameters']]$cost
coef0 = tuned[['best.parameters']]$coef0
model = svm(y~., data = train.set, probability=T,
kernel="sigmoid", gamma=gamma, cost=cost,coef0 = coef0,scale = FALSE)
}
prob = predict(model, newdata=test.set, probability=T)
prob
},
ada = {
model = ada(y~., data = train.set)
prob = predict(model, newdata=test.set, type='probs')
#print(cbind(prob,as.character(test.set$y)))
prob = prob[,2]/rowSums(prob)
prob
}
)
}
??mean
#Generate two bar charts, one for F-score and one for AUC,that allow for visually comparing different classificationtechniques.
#generate plot data
data.plot = data.frame(model = c('lr','knn1','knn3','knn5','nb','svm.radial','svm.sigmoid','dtree.default','dtree.prune'), auc = c(mat.lr[5],mat.knn1[5],mat.knn3[5],mat.knn5[5],mat.nb[5],mat.svm.radial[5],mat.svm.sigmoid[5],mat.dtree.default[5],mat.dtree.prune[5]))
ggplot(data.plot,aes(model,auc,fill = model)) + geom_bar(stat = "identity") + ggtitle("AUC Bar")
library(glmnet)#for penalize logistic regression
library("arm")
library(class) #for knn
library(MASS) # for the example dataset
library(plyr) # for recoding data
library(ROCR) # for plotting roc
library(e1071) # for NB and SVM
library(rpart) # for decision tree
library(ada) # for adaboost
library(ggplot2)
### main ###
dataset = load.data.pokemon()
dataset_new = load.data.clean(dataset)
#get best gamma and cost for svm
grid.search(dataset_new)
### main ###
dataset = load.data.pokemon()
dataset_new = load.data.clean(dataset)
#get best gamma and cost for svm
#grid.search(dataset_new)
#get all [predictions,actual]
pred.lr = k.fold.cv(dataset_new,'lr')
pred.knn1 = k.fold.cv(dataset_new,'knn',1)
pred.knn3 = k.fold.cv(dataset_new,'knn',3)
pred.knn5 = k.fold.cv(dataset_new,'knn',5)
pred.nb = k.fold.cv(dataset_new, 'nb')
pred.svm.radial = k.fold.cv(dataset_new, 'svm.radial',tune = T)
pred.svm.sigmoid = k.fold.cv(dataset_new, 'svm.sigmoid',tune = T)
pred.dtree.default = k.fold.cv(dataset_new, 'dtree')
pred.dtree.prune = k.fold.cv(dataset_new, 'dtree',tune = T)
pred.ada = k.fold.cv(dataset_new,'ada')
#get all models measure matrixes
mat.lr = get.measure.matrix(pred.lr,'lr')
mat.knn1 = get.measure.matrix(pred.knn1,'knn1')
mat.knn3 = get.measure.matrix(pred.knn3,'knn3')
mat.knn5 = get.measure.matrix(pred.knn5,'knn5')
mat.nb = get.measure.matrix(pred.nb,'nb')
mat.svm.radial = get.measure.matrix(pred.svm.radial,'svm_radial')
mat.svm.sigmoid = get.measure.matrix(pred.svm.sigmoid,'svm_sigmoid')
mat.dtree.default = get.measure.matrix(pred.dtree.default,'dtree_default')
mat.dtree.prune = get.measure.matrix(pred.dtree.prune,'dtree_prune')
mat.ada = get.measure.matrix(pred.ada,'ada')
#show measrure matrix
mat.all = cbind(mat.lr,mat.knn1,mat.knn3,mat.knn5,mat.nb,mat.svm.radial,mat.svm.sigmoid,mat.dtree.default,mat.dtree.prune,mat.ada)
mat.all
#Generate two bar charts, one for F-score and one for AUC,that allow for visually comparing different classificationtechniques.
#generate plot data
data.plot = data.frame(model = c('lr','knn1','knn3','knn5','nb','svm.radial','svm.sigmoid','dtree.default','dtree.prune','ada'), auc = c(mat.lr[5],mat.knn1[5],mat.knn3[5],mat.knn5[5],mat.nb[5],mat.svm.radial[5],mat.svm.sigmoid[5],mat.dtree.default[5],mat.dtree.prune[5],mat.ada[5]))
ggplot(data.plot,aes(model,auc,fill = model)) + geom_bar(stat = "identity") + ggtitle("AUC Bar")
data.plot = data.frame(model = c('lr','knn1','knn3','knn5','nb','svm.radial','svm.sigmoid','dtree.default','dtree.prune'), fscore = c(mat.lr[4],mat.knn1[4],mat.knn3[4],mat.knn5[4],mat.nb[4],mat.svm.radial[4],mat.svm.sigmoid[4],mat.dtree.default[4],mat.dtree.prune[4],mat.ada[5]))
data.plot = data.frame(model = c('lr','knn1','knn3','knn5','nb','svm.radial','svm.sigmoid','dtree.default','dtree.prune','ada'), fscore = c(mat.lr[4],mat.knn1[4],mat.knn3[4],mat.knn5[4],mat.nb[4],mat.svm.radial[4],mat.svm.sigmoid[4],mat.dtree.default[4],mat.dtree.prune[4],mat.ada[5]))
ggplot(data.plot,aes(model,fscore,fill = model)) + geom_bar(stat = "identity") + ggtitle("FScore")
#Generate an ROC plot that plot the ROC curve of each
#model into the same figure and include a legend to
#indicate the name of each curve. For techniques with
#variants, plot the best curve that has the highest AUC.
## plot ROC
#get all predictions with function get.predictions
preds_list = list(get.predictions(pred.lr),get.predictions(pred.knn3),get.predictions(pred.nb),get.predictions(pred.dtree.default),get.predictions(pred.svm.radial),get.predictions(pred.ada))
#generate acutal list with repeat ataset_new$y m(best model times)
m <- length(preds_list)
actuals_list <- rep(list(dataset_new$y), m)
#generate prediction object to plot ROC curve with package ROCR
predss = prediction(preds_list,actuals_list)
perf = performance(predss, "tpr","fpr")
#plot curve
plot(perf,col = as.list(1:m), main = "Test Set ROC Curves")
legend(x = "bottomright",c("lr","knn3","nb","dtree default","svm radial",'ada'),fill = 1 : m)
#Summarize the model performance based on the table and
#the ROC plot in one or two paragraphs.
data.plot = data.frame(model = c('lr','knn1','knn3','knn5','nb','svm.radial','svm.sigmoid','dtree.default','dtree.prune','ada'), fscore = c(mat.lr[4],mat.knn1[4],mat.knn3[4],mat.knn5[4],mat.nb[4],mat.svm.radial[4],mat.svm.sigmoid[4],mat.dtree.default[4],mat.dtree.prune[4],mat.ada[4]))
ggplot(data.plot,aes(model,fscore,fill = model)) + geom_bar(stat = "identity") + ggtitle("FScore")
dataset = load.data.pokemon()
dataset_new = load.data.clean(dataset)
View(dataset_new)
