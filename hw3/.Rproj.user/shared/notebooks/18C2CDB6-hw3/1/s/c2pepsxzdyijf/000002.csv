"0","do.classification <- function(train.set, test.set, "
"0","                              cl.name, knum,tune = F,verbose=F) {"
"0","  ## note: to plot ROC later, we want the raw probabilities,"
"0","  ## not binary decisions"
"0","  switch(cl.name, "
"0","         knn = {#set key by pass knum"
"0","             prob = knn(train.set[,-1], test.set[,-1], cl=train.set[,1], k = knum, prob=T)"
"0","             prob = attr(prob,""prob"")"
"0","             prob"
"0","         },"
"0","         lr = { # logistic regression"
"0","           #set glmnet for penalize logistic regression to avoid overfitting"
"0","           model = glmnet(as.matrix(train.set[-ncol(train.set)]),as.matrix(train.set[ncol(train.set)]),family=""binomial"",alpha=0,lambda=1/100,  standardize=F,thresh =1e-8,intercept = F)"
"0","           if (verbose) {"
"0","             print(summary(model))             "
"0","           }"
"0","            prob = predict(model, newx=as.matrix(test.set[-ncol(test.set)]), type=""response"") "
"0","           prob"
"0","         },"
"0","         nb = {#naive bayes model"
"0","           model = naiveBayes(y~., data=train.set)"
"0","           prob = predict(model, newdata=test.set, type=""raw"") "
"0","           prob = prob[,2]/rowSums(prob) # renormalize the prob."
"0","           prob"
"0","         },"
"0","         dtree = {"
"0","           model = rpart(y~., data=train.set)"
"0","           if (verbose) {"
"0","             print(summary(model)) # detailed summary of splits"
"0","             printcp(model) # print the cross-validation results"
"0","             plotcp(model) # visualize the cross-validation results"
"0","             ## plot the tree"
"0","             plot(model, uniform=TRUE, main=""Classification Tree"")"
"0","             text(model, use.n=TRUE, all=TRUE, cex=.8)"
"0","           }           "
"0","           prob = predict(model, newdata=test.set)"
"0","           "
"0","           if (tune) { "
"0","             ## prune the tree "
"0","             pfit<- prune(model, cp=model$cptable[which.min(model$cptable[,""xerror""]),""CP""])"
"0","             prob = predict(pfit, newdata=test.set)"
"0","             ## plot the pruned tree "
"0","             #plot(pfit, uniform=TRUE,main=""Pruned Classification Tree"")"
"0","             #text(pfit, use.n=TRUE, all=TRUE, cex=.8)             "
"0","           }"
"0","           #print(cbind(prob,as.character(test.set$y)))"
"0","           prob"
"0","         },"
"0","         svm.radial = {#svm with kernel radial"
"0","           #alredy get best gamma and cost in grid search"
"0","           model = svm(y~., data = train.set, probability=T, "
"0","                         kernel=""radial"", gamma=0.01, cost=10,scale = FALSE)    "
"0","           if (0) { # fine-tune the model with different kernel and parameters"
"0","             ## evaluate the range of gamma parameter between 0.000001 and 0.1"
"0","             ## and cost parameter from 0.1 until 10"
"0","             tuned <- tune.svm(y~., data = train.set, "
"0","                               kernel=""radial"", "
"0","                               gamma = 10^(-6:-1), cost = 10^(-1:2))"
"0","             print(summary(tuned))"
"0","             gamma = tuned[['best.parameters']]$gamma"
"0","             cost = tuned[['best.parameters']]$cost"
"0","             model = svm(y~., data = train.set, probability=T, "
"0","                         kernel=""radial"", gamma=gamma, cost=cost,scale = FALSE)                        "
"0","           }"
"0","           prob = predict(model, newdata=test.set, probability=T)"
"0","           prob"
"0","         },"
"0","         svm.sigmoid = {#svm with kernel sigmoid"
"0","           model = svm(y~., data=train.set, probability=T,scale = FALSE,kernel=""sigmoid"",gamma = 0.01, cost =10)"
"0","           if (0) { # fine-tune the model with different kernel and parameters"
"0","             ## evaluate the range of gamma parameter between 0.000001 and 0.1"
"0","             ## and cost parameter from 0.1 until 10"
"0","             tuned <- tune.svm(y~., data = train.set, "
"0","                               kernel=""sigmoid"", "
"0","                               gamma = 10^(-2:-1), cost = 10^(-1:2),coef0 = 10^(-1:1))"
"0","             #print(summary(tuned))"
"0","             gamma = tuned[['best.parameters']]$gamma"
"0","             cost = tuned[['best.parameters']]$cost"
"0","             coef0 = tuned[['best.parameters']]$coef0"
"0","             model = svm(y~., data = train.set, probability=T, "
"0","                         kernel=""sigmoid"", gamma=gamma, cost=cost,coef0 = coef0,scale = FALSE)                        "
"0","           }"
"0","           prob = predict(model, newdata=test.set, probability=T)"
"0","           prob"
"0","         },"
"0","         ada = {"
"0","           model = ada(y~., data = train.set)"
"0","           prob = predict(model, newdata=test.set, type='probs')"
"0","           #print(cbind(prob,as.character(test.set$y)))"
"0","           prob = prob[,2]/rowSums(prob)"
"0","           prob"
"0","         }"
"0","  ) "
"0","}"
